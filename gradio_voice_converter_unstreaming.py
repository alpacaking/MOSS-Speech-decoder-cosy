import gradio as gr
import torch
import torchaudio
import sys
import os
import tempfile
import numpy as np
import argparse  # æ–°å¢
from pathlib import Path
from torchaudio import transforms as T
import base64

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, 'GLM_modules'))
sys.path.append(current_dir)
from whisper_encoder_decoder import GLM4Encoder


# å…¨å±€å˜é‡
encoder = None
device = "cuda" if torch.cuda.is_available() else "cpu"

# å…¨å±€å˜é‡ç”¨äº tokenizer_path
TOKENIZER_PATH = None

# å…¨å±€å˜é‡ç”¨äºæ¨¡å‹è·¯å¾„
CONFIG_PATH = None
FEATURE_EXTRACTOR_PATH = None
FLOW_PATH = None

MAX_DURATION=90.0

# å…¨å±€å˜é‡é»˜è®¤å€¼ (å°†åœ¨ main ä¸­æ ¹æ®å‚æ•°æ›´æ–°)
OUTPUT_DIR = Path("./.gradio_outputs").resolve()

def initialize_model(mel_cache_len=8):
    """åˆå§‹åŒ–æ¨¡å‹"""
    global encoder
    if encoder is None:
        print("="*60)
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print("="*60)
        tokenizer_path = TOKENIZER_PATH
        feature_extractor_path = FEATURE_EXTRACTOR_PATH
        flow_path = FLOW_PATH
        encoder = GLM4Encoder(tokenizer_path=tokenizer_path, feature_extractor_path = feature_extractor_path, flow_path = flow_path,  mel_cache_len=mel_cache_len).to(device)
        encoder.eval()
        print("="*60)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"è®¾å¤‡: {device}")
        print(f"Mel cache length: {mel_cache_len}")
        print("="*60)
    return encoder



def calculate_rms(waveform):
    """è®¡ç®—éŸ³é¢‘çš„ RMS å€¼"""
    return torch.sqrt(torch.mean(waveform ** 2)).item()

def normalize_volume(waveform, target_rms):
    """å½’ä¸€åŒ–éŸ³é¢‘éŸ³é‡åˆ°ç›®æ ‡ RMS å€¼"""
    current_rms = torch.sqrt(torch.mean(waveform ** 2))
    if current_rms > 0:
        scale = target_rms / current_rms
        waveform = waveform * scale
    return waveform

def find_loudest_segment(waveform, sr, segment_duration, window_size=0.1):
    """
    æ‰¾å‡ºéŸ³é¢‘ä¸­éŸ³é‡æœ€å¤§çš„è¿ç»­ç‰‡æ®µ
    
    Args:
        waveform: éŸ³é¢‘æ³¢å½¢ [1, samples]
        sr: é‡‡æ ·ç‡
        segment_duration: ç›®æ ‡ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆç§’ï¼‰
    
    Returns:
        æˆªå–çš„éŸ³é¢‘ç‰‡æ®µ [1, samples]
    """
    if waveform.shape[1] <= segment_duration * sr:
        return waveform
    
    segment_samples = int(segment_duration * sr)
    window_samples = int(window_size * sr)
    audio_1d = waveform.squeeze(0)
    
    # ä½¿ç”¨è¾ƒå¤§çš„æ­¥é•¿æé«˜æ•ˆç‡
    hop_length = window_samples // 4
    energies = []
    for i in range(0, len(audio_1d) - window_samples + 1, hop_length):
        window = audio_1d[i:i + window_samples]
        energy = torch.sqrt(torch.mean(window ** 2))
        energies.append(energy.item())
    
    energies = np.array(energies)
    
    # å¹³æ»‘èƒ½é‡æ›²çº¿
    kernel_size = max(1, int(segment_duration / window_size))
    kernel = np.ones(kernel_size) / kernel_size
    if len(energies) >= kernel_size:
        smoothed_energies = np.convolve(energies, kernel, mode='valid')
    else:
        smoothed_energies = energies
    
    # æ‰¾åˆ°èƒ½é‡æœ€å¤§çš„ä½ç½®
    max_idx = np.argmax(smoothed_energies)
    start_sample = max_idx * hop_length
    end_sample = start_sample + segment_samples
    
    if end_sample > waveform.shape[1]:
        end_sample = waveform.shape[1]
        start_sample = max(0, end_sample - segment_samples)
    
    print(f"[INFO] æ‰¾åˆ°æœ€å“ç‰‡æ®µ: {start_sample/sr:.2f}s - {end_sample/sr:.2f}s")
    return waveform[:, start_sample:end_sample]


def calculate_rms(waveform):
    """è®¡ç®—éŸ³é¢‘çš„ RMS å€¼"""
    return torch.sqrt(torch.mean(waveform ** 2)).item()

def normalize_volume(waveform, target_rms):
    """å½’ä¸€åŒ–éŸ³é¢‘éŸ³é‡åˆ°ç›®æ ‡ RMS å€¼"""
    current_rms = torch.sqrt(torch.mean(waveform ** 2))
    if current_rms > 0:
        scale = target_rms / current_rms
        waveform = waveform * scale
    return waveform

def find_loudest_segment(waveform, sr, segment_duration, window_size=0.1):
    """æ‰¾å‡ºéŸ³é¢‘ä¸­éŸ³é‡æœ€å¤§çš„è¿ç»­ç‰‡æ®µ"""
    if waveform.shape[1] <= segment_duration * sr:
        return waveform
    segment_samples = int(segment_duration * sr)
    window_samples = int(window_size * sr)
    audio_1d = waveform.squeeze(0)
    hop_length = window_samples // 4
    energies = []
    for i in range(0, len(audio_1d) - window_samples + 1, hop_length):
        window = audio_1d[i:i + window_samples]
        energy = torch.sqrt(torch.mean(window ** 2))
        energies.append(energy.item())
    energies = np.array(energies)
    kernel_size = max(1, int(segment_duration / window_size))
    kernel = np.ones(kernel_size) / kernel_size
    if len(energies) >= kernel_size:
        smoothed_energies = np.convolve(energies, kernel, mode='valid')
    else:
        smoothed_energies = energies
    max_idx = np.argmax(smoothed_energies)
    start_sample = max_idx * hop_length
    end_sample = start_sample + segment_samples
    if end_sample > waveform.shape[1]:
        end_sample = waveform.shape[1]
        start_sample = max(0, end_sample - segment_samples)
    print(f"[INFO] æ‰¾åˆ°æœ€å“ç‰‡æ®µ: {start_sample/sr:.2f}s - {end_sample/sr:.2f}s")
    return waveform[:, start_sample:end_sample]


def calculate_rms(waveform):
    """è®¡ç®—éŸ³é¢‘çš„ RMS å€¼"""
    return torch.sqrt(torch.mean(waveform ** 2)).item()

def normalize_volume(waveform, target_rms):
    """å½’ä¸€åŒ–éŸ³é¢‘éŸ³é‡åˆ°ç›®æ ‡ RMS å€¼"""
    current_rms = torch.sqrt(torch.mean(waveform ** 2))
    if current_rms > 0:
        scale = target_rms / current_rms
        waveform = waveform * scale
    return waveform

def find_loudest_segment(waveform, sr, segment_duration, window_size=0.1):
    """æ‰¾å‡ºéŸ³é¢‘ä¸­éŸ³é‡æœ€å¤§çš„è¿ç»­ç‰‡æ®µ"""
    if waveform.shape[1] <= segment_duration * sr:
        return waveform
    segment_samples = int(segment_duration * sr)
    window_samples = int(window_size * sr)
    audio_1d = waveform.squeeze(0)
    hop_length = window_samples // 4
    energies = []
    for i in range(0, len(audio_1d) - window_samples + 1, hop_length):
        window = audio_1d[i:i + window_samples]
        energy = torch.sqrt(torch.mean(window ** 2))
        energies.append(energy.item())
    energies = np.array(energies)
    kernel_size = max(1, int(segment_duration / window_size))
    kernel = np.ones(kernel_size) / kernel_size
    if len(energies) >= kernel_size:
        smoothed_energies = np.convolve(energies, kernel, mode='valid')
    else:
        smoothed_energies = energies
    max_idx = np.argmax(smoothed_energies)
    start_sample = max_idx * hop_length
    end_sample = start_sample + segment_samples
    if end_sample > waveform.shape[1]:
        end_sample = waveform.shape[1]
        start_sample = max(0, end_sample - segment_samples)
    print(f"[INFO] æ‰¾åˆ°æœ€å“ç‰‡æ®µ: {start_sample/sr:.2f}s - {end_sample/sr:.2f}s")
    return waveform[:, start_sample:end_sample]

def process_gradio_audio(audio_data, max_duration=30.0):
    """
    å¤„ç† Gradio éŸ³é¢‘æ•°æ®ï¼Œå‚è€ƒ stable-audio çš„å¤„ç†æ–¹å¼
    
    Args:
        audio_data: tuple (sample_rate, audio_array)
        max_duration: æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
    
    Returns:
        tuple: (torch.Tensor, sample_rate, duration) å¤„ç†åçš„éŸ³é¢‘å¼ é‡ [channels, samples]
    """
    sample_rate, audio_array = audio_data
    
    print(f"[DEBUG] åŸå§‹éŸ³é¢‘ä¿¡æ¯:")
    print(f"  - Sample rate: {sample_rate}")
    print(f"  - Array shape: {audio_array.shape}")
    print(f"  - Array dtype: {audio_array.dtype}")
    print(f"  - Array range: [{audio_array.min():.6f}, {audio_array.max():.6f}]")
    
    # æ ¹æ® dtype è½¬æ¢ä¸º torch tensor (å‚è€ƒ stable-audio)
    if audio_array.dtype == np.float32:
        audio = torch.from_numpy(audio_array)
    elif audio_array.dtype == np.int16:
        audio = torch.from_numpy(audio_array).float().div(32767)
    elif audio_array.dtype == np.int32:
        audio = torch.from_numpy(audio_array).float().div(2147483647)
    else:
        raise ValueError(f"Unsupported audio data type: {audio_array.dtype}")
    
    # å¤„ç†ç»´åº¦
    if audio.dim() == 1:
        audio = audio.unsqueeze(0)  # [1, n]
    elif audio.dim() == 2:
        audio = audio.transpose(0, 1)  # [n, 2] -> [2, n]
    
    # å¦‚æœæ˜¯å¤šå£°é“ï¼Œå–å¹³å‡åˆ°å•å£°é“
    if audio.shape[0] > 1:
        print(f"[INFO] æ£€æµ‹åˆ° {audio.shape[0]} å£°é“éŸ³é¢‘ï¼Œè½¬æ¢ä¸ºå•å£°é“")
        audio = audio.mean(dim=0, keepdim=True)
    
    # è®¡ç®—éŸ³é¢‘æ—¶é•¿
    duration = audio.shape[1] / sample_rate
    print(f"[INFO] éŸ³é¢‘æ—¶é•¿: {duration:.2f} ç§’")
    
    # é™åˆ¶æœ€å¤§æ—¶é•¿
    if max_duration is not None and duration > max_duration:
        max_samples = int(max_duration * sample_rate)
        audio = audio[:, :max_samples]
        print(f"[WARNING] éŸ³é¢‘è¶…è¿‡æœ€å¤§æ—¶é•¿ {max_duration}ç§’ï¼Œå·²æˆªæ–­åˆ° {max_duration}ç§’")
        duration = max_duration
    
    print(f"[DEBUG] è½¬æ¢åéŸ³é¢‘:")
    print(f"  - Tensor shape: {audio.shape}")
    print(f"  - Tensor range: [{audio.min():.6f}, {audio.max():.6f}]")
    print(f"  - Duration: {duration:.2f}s")
    
    return audio, sample_rate, duration


def save_audio_for_gradio(audio_tensor, sample_rate, prefix="output"):
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶ä¾› Gradio ä½¿ç”¨
    å‚è€ƒ stable-audio çš„ä¿å­˜æ–¹å¼
    """
    try:
        # ç¡®ä¿éŸ³é¢‘æ˜¯ 2D å¼ é‡ [channels, samples]
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        
        # è½¬æ¢ä¸º int16 æ ¼å¼ (å‚è€ƒ stable-audio)
        # audio.to(torch.float32).div(torch.max(torch.abs(audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()
        audio_normalized = audio_tensor.to(torch.float32)
        
        # å½’ä¸€åŒ–åˆ° [-1, 1]
        max_val = torch.max(torch.abs(audio_normalized))
        if max_val > 0:
            audio_normalized = audio_normalized.div(max_val)
        audio_normalized = audio_normalized.clamp(-1, 1)
        
        # è½¬æ¢ä¸º int16
        audio_int16 = audio_normalized.mul(32767).to(torch.int16).cpu()
        
        # ä½¿ç”¨æ—¶é—´æˆ³é˜²æ­¢æµè§ˆå™¨ç¼“å­˜
        import time
        timestamp = int(time.time() * 1000)
        output_path = OUTPUT_DIR / f"{prefix}_{timestamp}.wav"
        
        print(f"[INFO] ä¿å­˜éŸ³é¢‘åˆ°: {output_path}")
        print(f"[INFO] éŸ³é¢‘å½¢çŠ¶: {audio_int16.shape}")
        print(f"[INFO] é‡‡æ ·ç‡: {sample_rate}")
        print(f"[INFO] éŸ³é¢‘èŒƒå›´: [{audio_int16.min()}, {audio_int16.max()}]")
        
        torchaudio.save(
            str(output_path),
            audio_int16,
            sample_rate=sample_rate
        )
        
        print(f"[SUCCESS] éŸ³é¢‘å·²ä¿å­˜: {output_path}")
        
        # éªŒè¯æ–‡ä»¶
        if output_path.exists():
            file_size = output_path.stat().st_size
            print(f"[INFO] æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # è¯»å–éªŒè¯
            try:
                verify_audio, verify_sr = torchaudio.load(str(output_path))
                print(f"[INFO] éªŒè¯è¯»å–æˆåŠŸ: shape={verify_audio.shape}, sr={verify_sr}")
            except Exception as e:
                print(f"[ERROR] éªŒè¯è¯»å–å¤±è´¥: {e}")
        
        return str(output_path)
        
    except Exception as e:
        print(f"[ERROR] ä¿å­˜éŸ³é¢‘å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def load_audio_for_frontend(audio_path: str):
    if not audio_path or not os.path.exists(audio_path):
        print(f"[WARN] æ— æ³•æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        return None
    waveform, sample_rate = torchaudio.load(audio_path)
    waveform = waveform.to(torch.float32)
    if waveform.dim() > 1 and waveform.size(0) == 1:
        waveform = waveform.squeeze(0)
    return sample_rate, waveform.cpu().numpy()

def get_audio_html(file_path):
    """å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸º HTML æ’­æ”¾å™¨ä»£ç """
    if not file_path or not os.path.exists(file_path):
        return "<div>æ— éŸ³é¢‘æ–‡ä»¶</div>"
    
    try:
        with open(file_path, "rb") as f:
            b64_data = base64.b64encode(f.read()).decode('utf-8')
            
        filename = os.path.basename(file_path)
        return f"""
        <div style="padding: 10px; border: 1px solid #ddd; border-radius: 8px; background: #fafafa;">
            <div style="margin-bottom: 8px; font-size: 0.9em; color: #666;">ğŸ“„ {filename}</div>
            <audio controls style="width: 100%">
                <source src="data:audio/wav;base64,{b64_data}" type="audio/wav">
            </audio>
            <div style="margin-top: 5px; text-align: right;">
                <a href="data:audio/wav;base64,{b64_data}" download="{filename}" target="_blank">â¬‡ï¸ ä¸‹è½½</a>
            </div>
        </div>
        """
    except Exception as e:
        return f"<div>åŠ è½½å¤±è´¥: {str(e)}</div>"

def reload_audio(audio_path: str):
    return get_audio_html(audio_path)

def process_audio_nonstreaming(
    input_audio,
    reference_audio,
    reference_ratio = 0.8,
    use_spk_embedding = True,
    use_prompt_speech = True,
    mel_cache_len = 8
):
    """éæµå¼éŸ³é¢‘å¤„ç†"""
    try:
        print("\n" + "="*60)
        print("å¼€å§‹éæµå¼å¤„ç†")
        print("="*60)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = initialize_model(mel_cache_len=mel_cache_len)
        
        if input_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ è¾“å…¥éŸ³é¢‘", None
        if reference_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘", None
        
        # å¤„ç†è¾“å…¥éŸ³é¢‘ï¼ˆé™åˆ¶æœ€å¤§30ç§’ï¼‰
        print("\n[STEP 1] å¤„ç†è¾“å…¥éŸ³é¢‘...")
        input_tensor, input_sr, input_duration = process_gradio_audio(input_audio, max_duration=MAX_DURATION)
        input_rms = calculate_rms(input_tensor)
        input_rms = calculate_rms(input_tensor)
        temp_input = OUTPUT_DIR / "temp_input.wav"
        torchaudio.save(str(temp_input), input_tensor, input_sr)
        print(f"[INFO] ä¸´æ—¶è¾“å…¥æ–‡ä»¶å·²ä¿å­˜: {temp_input}")
        print(f"[INFO] è¾“å…¥éŸ³é¢‘ RMS: {input_rms:.6f}")
        print(f"[INFO] è¾“å…¥éŸ³é¢‘ RMS: {input_rms:.6f}")
        
        # å¤„ç†å‚è€ƒéŸ³é¢‘ï¼ˆæ™ºèƒ½æˆªå– + éŸ³é‡å½’ä¸€åŒ–ï¼‰
        print("\n[STEP 2] å¤„ç†å‚è€ƒéŸ³é¢‘...")
        ref_tensor, ref_sr, ref_duration = process_gradio_audio(reference_audio, max_duration=10.0)
        
        # æ™ºèƒ½æˆªå–æœ€å“ç‰‡æ®µ
        target_duration = reference_ratio * min(ref_duration, 10.0)
        if ref_duration <= target_duration:
            print(f"[INFO] å‚è€ƒéŸ³é¢‘æ—¶é•¿ {ref_duration:.2f}s <= ç›®æ ‡æ—¶é•¿ {target_duration:.2f}sï¼Œä¸éœ€è¦æˆªå–")
            ref_segment = ref_tensor
        else:
            ref_segment = find_loudest_segment(ref_tensor, ref_sr, target_duration)
        
        # éŸ³é‡å½’ä¸€åŒ–
        ref_rms_before = calculate_rms(ref_segment)
        ref_segment = normalize_volume(ref_segment, input_rms)
        ref_rms_after = calculate_rms(ref_segment)
        print(f"[INFO] å‚è€ƒéŸ³é¢‘ RMS: {ref_rms_before:.6f} -> {ref_rms_after:.6f}")
        
        ref_duration_final = ref_segment.shape[1] / ref_sr
        temp_reference = OUTPUT_DIR / "temp_reference.wav"
        torchaudio.save(str(temp_reference), ref_segment, ref_sr)
        print(f"[INFO] ä¸´æ—¶å‚è€ƒæ–‡ä»¶å·²ä¿å­˜: {temp_reference}")
        print(f"[INFO] å‚è€ƒéŸ³é¢‘å¤„ç†åæ—¶é•¿: {ref_duration_final:.2f}s")
        
        # ç¼–ç è¾“å…¥éŸ³é¢‘
        print("\n[STEP 3] æ­£åœ¨ç¼–ç éŸ³é¢‘...")
        audio_tokens = model.encode_token(str(temp_input))
        print(f"[INFO] âœ… ç”Ÿæˆäº† {len(audio_tokens)} ä¸ª tokens")
        
        # éæµå¼è§£ç 
        print(f"\n[STEP 4] æ­£åœ¨è¿›è¡Œéæµå¼è§£ç ...")
        print(f"[INFO] å‚æ•°: use_spk_embedding={use_spk_embedding}, use_prompt_speech={use_prompt_speech}")
        
        result = model.decode(
            [audio_tokens],
            prompt_speech=str(temp_reference),
            use_spk_embedding=use_spk_embedding,
            use_prompt_speech=use_prompt_speech,
            device=device
        )
        
        # ä¿å­˜è¾“å‡º
        print(f"\n[STEP 5] ä¿å­˜è¾“å‡ºéŸ³é¢‘...")
        output_audio = result['syn_wav_list'][0]
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘å½¢çŠ¶: {output_audio.shape}")
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘èŒƒå›´: [{output_audio.min():.6f}, {output_audio.max():.6f}]")
        
        output_path = save_audio_for_gradio(
            output_audio,
            sample_rate=24000,
            prefix="nonstreaming_output"
        )
                
        if output_path is None:
            return None, "âŒ ä¿å­˜éŸ³é¢‘å¤±è´¥", None
        
        info = (
            "âœ… éæµå¼è§£ç å®Œæˆ\n"
            f"è¾“å…¥éŸ³é¢‘æ—¶é•¿: {input_duration:.2f}ç§’\n"
            f"å‚è€ƒéŸ³é¢‘æ—¶é•¿: {ref_duration_final:.2f}ç§’\n"
            f"å‚è€ƒéŸ³é¢‘æˆªå–æ¯”ä¾‹: {reference_ratio}\n"
            f"Token æ•°é‡: {len(audio_tokens)}\n"
            f"ä½¿ç”¨è¯´è¯äººåµŒå…¥: {use_spk_embedding}\n"
            f"ä½¿ç”¨æç¤ºè¯­éŸ³: {use_prompt_speech}\n"
            f"Mel cache length: {mel_cache_len}\n"
            f"è¾“å‡ºæ–‡ä»¶: {output_path}"
        )
        
        print("\n" + "="*60)
        print(info)
        print("="*60 + "\n")
        
        # ä¿®æ”¹ï¼šç›´æ¥è¿”å› output_path
        return get_audio_html(output_path), info, output_path        
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        print("\n" + "="*60)
        print(error_msg)
        print("="*60 + "\n")
        return None, error_msg, None



def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    # å…ˆåˆå§‹åŒ–æ¨¡å‹
    model = initialize_model(mel_cache_len=8)
    
    with gr.Blocks(title="Streaming Codec å˜å£°å™¨æµ‹è¯•", theme=gr.themes.Soft()) as demo:
        gr.HTML(
            """
            <div style='text-align: center'>
                <h1>ğŸ™ï¸ Streaming Codec å˜å£°å™¨æµ‹è¯•</h1>
                <p>ä¸Šä¼ è¾“å…¥éŸ³é¢‘å’Œå‚è€ƒéŸ³é¢‘ï¼Œæµ‹è¯•éæµå¼å’Œæµå¼è§£ç æ•ˆæœ</p>
            </div>
            """
        )
        
        nonstream_state = gr.State(value=None)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ è¾“å…¥è®¾ç½®")
                
                input_audio = gr.Audio(
                    label="è¾“å…¥éŸ³é¢‘ (å¾…è½¬æ¢çš„éŸ³é¢‘)",
                    type="numpy",
                    sources=["upload", "microphone"]
                )
                
                reference_audio = gr.Audio(
                    label="å‚è€ƒéŸ³é¢‘ (ç›®æ ‡éŸ³è‰²)",
                    type="numpy",
                    sources=["upload", "microphone"]
                )
                
                gr.Markdown("### âš™ï¸ å‚æ•°è®¾ç½®")
                
                reference_ratio = gr.Slider(
                    minimum=0.3,
                    maximum=1.0,
                    step=0.1,
                    value=0.8,
                    label="å‚è€ƒéŸ³é¢‘æˆªå–æ¯”ä¾‹",
                    info="ä»å‚è€ƒéŸ³é¢‘ä¸­æˆªå–æœ€å“éƒ¨åˆ†çš„æ¯”ä¾‹ (0.3-1.0)"
                )
                
                mel_cache_len = 8
                use_spk_embedding = True
                use_prompt_speech = True
                # mel_cache_len = gr.Slider(
                #     minimum=4,
                #     maximum=16,
                #     step=1,
                #     value=8,
                #     label="Mel Cache Length",
                #     info="Vocoderè§£ç æ—¶overlapçš„é•¿åº¦ï¼Œå½±å“éŸ³è´¨è¿ç»­æ€§"
                # )
                
                # use_spk_embedding = gr.Checkbox(
                #     label="ä½¿ç”¨è¯´è¯äººåµŒå…¥",
                #     value=True
                # )
                # use_prompt_speech = gr.Checkbox(
                #     label="ä½¿ç”¨æç¤ºè¯­éŸ³",
                #     value=True
                # )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸµ éæµå¼è§£ç ")
                
                with gr.Row():
                    nonstream_button = gr.Button(
                        "ğŸš€ éæµå¼è§£ç ",
                        variant="primary",
                        size="lg"
                    )
                
                nonstream_output = gr.HTML(label="éæµå¼è¾“å‡º")
                nonstream_reload = gr.Button("ğŸ” é‡æ–°åŠ è½½éæµå¼éŸ³é¢‘")
                nonstream_info = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=8,
                    interactive=False
                )
        
        # ç»‘å®šäº‹ä»¶
        nonstream_button.click(
            fn=process_audio_nonstreaming,
            inputs=[input_audio, reference_audio, reference_ratio],
            outputs=[nonstream_output, nonstream_info, nonstream_state]
        )


        nonstream_reload.click(
            fn=reload_audio,
            inputs=[nonstream_state],
            outputs=[nonstream_output]
        )
        
        # ä½¿ç”¨æç¤º
        gr.Markdown("### ğŸ’¡ ä½¿ç”¨æç¤º")
        gr.Markdown(f"""
        1. **è¾“å…¥éŸ³é¢‘**: ä½ æƒ³è¦è½¬æ¢çš„åŸå§‹éŸ³é¢‘
        2. **å‚è€ƒéŸ³é¢‘**: ç›®æ ‡éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘
        3. **Mel Cache Length**: Vocoderè§£ç æ—¶overlapçš„é•¿åº¦ï¼Œå»ºè®® mel_cache_len / 4 â‰¤ block_size
        4. **éæµå¼è§£ç **: ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªéŸ³é¢‘ï¼Œè´¨é‡æ›´å¥½
        8. **è¾“å‡ºç›®å½•**: `{OUTPUT_DIR}`
        
        **é‡è¦æç¤º**: 
        - prompt éŸ³é¢‘ä¼šè¢«æˆªæ–­è‡³å‰10sã€‚è¯·æä¾›æ¸…æ™°çš„ prompt éŸ³é¢‘
        """)
    
    return demo


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Streaming Codec Gradio Demo")
    parser.add_argument("--output_dir", type=str, default=None, help="æŒ‡å®šéŸ³é¢‘è¾“å‡ºç›®å½•")
    parser.add_argument("--port", type=int, default=7860, help="æŒ‡å®šæœåŠ¡ç«¯å£")
    parser.add_argument("--tokenizer_path", type=str, default="./SpeechTokenizerTrainer_final/generator_ckpt", help="Path to tokenizer checkpoint")
    parser.add_argument("--config_path", type=str, default=None, help="Path to config.json (optional)")
    parser.add_argument("--feature_extractor_path", type=str, default=None, help="Path to glm-4-voice-tokenizer (optional)")
    parser.add_argument("--flow_path", type=str, default=None, help="Path to flow directory (optional)")

    args = parser.parse_args()
    
    TOKENIZER_PATH = args.tokenizer_path
    CONFIG_PATH = args.config_path
    FEATURE_EXTRACTOR_PATH = args.feature_extractor_path
    FLOW_PATH = args.flow_path

    # ç¡®å®šè¾“å‡ºç›®å½•: å‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
    if args.output_dir:
        OUTPUT_DIR = Path(args.output_dir).resolve()
    elif os.getenv("GRADIO_OUTPUT_DIR"):
        OUTPUT_DIR = Path(os.getenv("GRADIO_OUTPUT_DIR")).resolve()
    
    # åˆ›å»ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½• (ç»å¯¹è·¯å¾„): {OUTPUT_DIR}")
    print(f"æœåŠ¡ç«¯å£: {args.port}")

    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=False,
        allowed_paths=[str(OUTPUT_DIR)]
    )
    
    
    
    
