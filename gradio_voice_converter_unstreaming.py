import gradio as gr
import torch
import torchaudio
import sys
import os
import tempfile
import numpy as np
import argparse  # æ–°å¢
from pathlib import Path
from torchaudio import transforms as T
import base64

sys.path.append('/inspire/hdd/project/embodied-multimodality/public/lzjjin/Streaming-Codec/GLM_modules')
sys.path.append('/inspire/hdd/project/embodied-multimodality/public/lzjjin/Streaming-Codec')
from whisper_encoder_decoder import GLM4Encoder


# å…¨å±€å˜é‡
encoder = None
device = "cuda" if torch.cuda.is_available() else "cpu"

# å…¨å±€å˜é‡é»˜è®¤å€¼ (å°†åœ¨ main ä¸­æ ¹æ®å‚æ•°æ›´æ–°)
OUTPUT_DIR = Path("./.gradio_outputs").resolve()

def initialize_model(mel_cache_len=8):
    """åˆå§‹åŒ–æ¨¡å‹"""
    global encoder
    if encoder is None:
        print("="*60)
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        print("="*60)
        tokenizer_path = '/inspire/hdd/project/embodied-multimodality/public/lzjjin/Streaming-Codec/SpeechTokenizerTrainer_final/generator_ckpt'
        encoder = GLM4Encoder(tokenizer_path=tokenizer_path, mel_cache_len=mel_cache_len).to(device)
        encoder.eval()
        print("="*60)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"è®¾å¤‡: {device}")
        print(f"Mel cache length: {mel_cache_len}")
        print("="*60)
    return encoder


def process_gradio_audio(audio_data, max_duration=30.0):
    """
    å¤„ç† Gradio éŸ³é¢‘æ•°æ®ï¼Œå‚è€ƒ stable-audio çš„å¤„ç†æ–¹å¼
    
    Args:
        audio_data: tuple (sample_rate, audio_array)
        max_duration: æœ€å¤§éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’
    
    Returns:
        tuple: (torch.Tensor, sample_rate, duration) å¤„ç†åçš„éŸ³é¢‘å¼ é‡ [channels, samples]
    """
    sample_rate, audio_array = audio_data
    
    print(f"[DEBUG] åŸå§‹éŸ³é¢‘ä¿¡æ¯:")
    print(f"  - Sample rate: {sample_rate}")
    print(f"  - Array shape: {audio_array.shape}")
    print(f"  - Array dtype: {audio_array.dtype}")
    print(f"  - Array range: [{audio_array.min():.6f}, {audio_array.max():.6f}]")
    
    # æ ¹æ® dtype è½¬æ¢ä¸º torch tensor (å‚è€ƒ stable-audio)
    if audio_array.dtype == np.float32:
        audio = torch.from_numpy(audio_array)
    elif audio_array.dtype == np.int16:
        audio = torch.from_numpy(audio_array).float().div(32767)
    elif audio_array.dtype == np.int32:
        audio = torch.from_numpy(audio_array).float().div(2147483647)
    else:
        raise ValueError(f"Unsupported audio data type: {audio_array.dtype}")
    
    # å¤„ç†ç»´åº¦
    if audio.dim() == 1:
        audio = audio.unsqueeze(0)  # [1, n]
    elif audio.dim() == 2:
        audio = audio.transpose(0, 1)  # [n, 2] -> [2, n]
    
    # å¦‚æœæ˜¯å¤šå£°é“ï¼Œå–å¹³å‡åˆ°å•å£°é“
    if audio.shape[0] > 1:
        print(f"[INFO] æ£€æµ‹åˆ° {audio.shape[0]} å£°é“éŸ³é¢‘ï¼Œè½¬æ¢ä¸ºå•å£°é“")
        audio = audio.mean(dim=0, keepdim=True)
    
    # è®¡ç®—éŸ³é¢‘æ—¶é•¿
    duration = audio.shape[1] / sample_rate
    print(f"[INFO] éŸ³é¢‘æ—¶é•¿: {duration:.2f} ç§’")
    
    # é™åˆ¶æœ€å¤§æ—¶é•¿
    if duration > max_duration:
        max_samples = int(max_duration * sample_rate)
        audio = audio[:, :max_samples]
        print(f"[WARNING] éŸ³é¢‘è¶…è¿‡æœ€å¤§æ—¶é•¿ {max_duration}ç§’ï¼Œå·²æˆªæ–­åˆ° {max_duration}ç§’")
        duration = max_duration
    
    print(f"[DEBUG] è½¬æ¢åéŸ³é¢‘:")
    print(f"  - Tensor shape: {audio.shape}")
    print(f"  - Tensor range: [{audio.min():.6f}, {audio.max():.6f}]")
    print(f"  - Duration: {duration:.2f}s")
    
    return audio, sample_rate, duration


def save_audio_for_gradio(audio_tensor, sample_rate, prefix="output"):
    """
    ä¿å­˜éŸ³é¢‘æ–‡ä»¶ä¾› Gradio ä½¿ç”¨
    å‚è€ƒ stable-audio çš„ä¿å­˜æ–¹å¼
    """
    try:
        # ç¡®ä¿éŸ³é¢‘æ˜¯ 2D å¼ é‡ [channels, samples]
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        
        # è½¬æ¢ä¸º int16 æ ¼å¼ (å‚è€ƒ stable-audio)
        # audio.to(torch.float32).div(torch.max(torch.abs(audio))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()
        audio_normalized = audio_tensor.to(torch.float32)
        
        # å½’ä¸€åŒ–åˆ° [-1, 1]
        max_val = torch.max(torch.abs(audio_normalized))
        if max_val > 0:
            audio_normalized = audio_normalized.div(max_val)
        audio_normalized = audio_normalized.clamp(-1, 1)
        
        # è½¬æ¢ä¸º int16
        audio_int16 = audio_normalized.mul(32767).to(torch.int16).cpu()
        
        # ä½¿ç”¨æ—¶é—´æˆ³é˜²æ­¢æµè§ˆå™¨ç¼“å­˜
        import time
        timestamp = int(time.time() * 1000)
        output_path = OUTPUT_DIR / f"{prefix}_{timestamp}.wav"
        
        print(f"[INFO] ä¿å­˜éŸ³é¢‘åˆ°: {output_path}")
        print(f"[INFO] éŸ³é¢‘å½¢çŠ¶: {audio_int16.shape}")
        print(f"[INFO] é‡‡æ ·ç‡: {sample_rate}")
        print(f"[INFO] éŸ³é¢‘èŒƒå›´: [{audio_int16.min()}, {audio_int16.max()}]")
        
        torchaudio.save(
            str(output_path),
            audio_int16,
            sample_rate=sample_rate
        )
        
        print(f"[SUCCESS] éŸ³é¢‘å·²ä¿å­˜: {output_path}")
        
        # éªŒè¯æ–‡ä»¶
        if output_path.exists():
            file_size = output_path.stat().st_size
            print(f"[INFO] æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # è¯»å–éªŒè¯
            try:
                verify_audio, verify_sr = torchaudio.load(str(output_path))
                print(f"[INFO] éªŒè¯è¯»å–æˆåŠŸ: shape={verify_audio.shape}, sr={verify_sr}")
            except Exception as e:
                print(f"[ERROR] éªŒè¯è¯»å–å¤±è´¥: {e}")
        
        return str(output_path)
        
    except Exception as e:
        print(f"[ERROR] ä¿å­˜éŸ³é¢‘å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def load_audio_for_frontend(audio_path: str):
    if not audio_path or not os.path.exists(audio_path):
        print(f"[WARN] æ— æ³•æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        return None
    waveform, sample_rate = torchaudio.load(audio_path)
    waveform = waveform.to(torch.float32)
    if waveform.dim() > 1 and waveform.size(0) == 1:
        waveform = waveform.squeeze(0)
    return sample_rate, waveform.cpu().numpy()

def get_audio_html(file_path):
    """å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸º HTML æ’­æ”¾å™¨ä»£ç """
    if not file_path or not os.path.exists(file_path):
        return "<div>æ— éŸ³é¢‘æ–‡ä»¶</div>"
    
    try:
        with open(file_path, "rb") as f:
            b64_data = base64.b64encode(f.read()).decode('utf-8')
            
        filename = os.path.basename(file_path)
        return f"""
        <div style="padding: 10px; border: 1px solid #ddd; border-radius: 8px; background: #fafafa;">
            <div style="margin-bottom: 8px; font-size: 0.9em; color: #666;">ğŸ“„ {filename}</div>
            <audio controls style="width: 100%">
                <source src="data:audio/wav;base64,{b64_data}" type="audio/wav">
            </audio>
            <div style="margin-top: 5px; text-align: right;">
                <a href="data:audio/wav;base64,{b64_data}" download="{filename}" target="_blank">â¬‡ï¸ ä¸‹è½½</a>
            </div>
        </div>
        """
    except Exception as e:
        return f"<div>åŠ è½½å¤±è´¥: {str(e)}</div>"

def reload_audio(audio_path: str):
    return get_audio_html(audio_path)

def process_audio_nonstreaming(
    input_audio,
    reference_audio,
    use_spk_embedding = True,
    use_prompt_speech = True,
    mel_cache_len = 8
):
    """éæµå¼éŸ³é¢‘å¤„ç†"""
    try:
        print("\n" + "="*60)
        print("å¼€å§‹éæµå¼å¤„ç†")
        print("="*60)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = initialize_model(mel_cache_len=mel_cache_len)
        
        if input_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ è¾“å…¥éŸ³é¢‘", None
        if reference_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘", None
        
        # å¤„ç†è¾“å…¥éŸ³é¢‘ï¼ˆé™åˆ¶æœ€å¤§30ç§’ï¼‰
        print("\n[STEP 1] å¤„ç†è¾“å…¥éŸ³é¢‘...")
        input_tensor, input_sr, input_duration = process_gradio_audio(input_audio, max_duration=30.0)
        temp_input = OUTPUT_DIR / "temp_input.wav"
        torchaudio.save(str(temp_input), input_tensor, input_sr)
        print(f"[INFO] ä¸´æ—¶è¾“å…¥æ–‡ä»¶å·²ä¿å­˜: {temp_input}")
        
        # å¤„ç†å‚è€ƒéŸ³é¢‘ï¼ˆé™åˆ¶æœ€å¤§10ç§’ï¼‰
        print("\n[STEP 2] å¤„ç†å‚è€ƒéŸ³é¢‘...")
        ref_tensor, ref_sr, ref_duration = process_gradio_audio(reference_audio, max_duration=10.0)
        temp_reference = OUTPUT_DIR / "temp_reference.wav"
        torchaudio.save(str(temp_reference), ref_tensor, ref_sr)
        print(f"[INFO] ä¸´æ—¶å‚è€ƒæ–‡ä»¶å·²ä¿å­˜: {temp_reference}")
        
        # ç¼–ç è¾“å…¥éŸ³é¢‘
        print("\n[STEP 3] æ­£åœ¨ç¼–ç éŸ³é¢‘...")
        audio_tokens = model.encode_token(str(temp_input))
        print(f"[INFO] âœ… ç”Ÿæˆäº† {len(audio_tokens)} ä¸ª tokens")
        
        # éæµå¼è§£ç 
        print(f"\n[STEP 4] æ­£åœ¨è¿›è¡Œéæµå¼è§£ç ...")
        print(f"[INFO] å‚æ•°: use_spk_embedding={use_spk_embedding}, use_prompt_speech={use_prompt_speech}")
        
        result = model.decode(
            [audio_tokens],
            prompt_speech=str(temp_reference),
            use_spk_embedding=use_spk_embedding,
            use_prompt_speech=use_prompt_speech,
            device=device
        )
        
        # ä¿å­˜è¾“å‡º
        print(f"\n[STEP 5] ä¿å­˜è¾“å‡ºéŸ³é¢‘...")
        output_audio = result['syn_wav_list'][0]
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘å½¢çŠ¶: {output_audio.shape}")
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘èŒƒå›´: [{output_audio.min():.6f}, {output_audio.max():.6f}]")
        
        output_path = save_audio_for_gradio(
            output_audio,
            sample_rate=24000,
            prefix="nonstreaming_output"
        )
                
        if output_path is None:
            return None, "âŒ ä¿å­˜éŸ³é¢‘å¤±è´¥", None
        
        info = (
            "âœ… éæµå¼è§£ç å®Œæˆ\n"
            f"è¾“å…¥éŸ³é¢‘æ—¶é•¿: {input_duration:.2f}ç§’\n"
            f"å‚è€ƒéŸ³é¢‘æ—¶é•¿: {ref_duration:.2f}ç§’\n"
            f"Token æ•°é‡: {len(audio_tokens)}\n"
            f"ä½¿ç”¨è¯´è¯äººåµŒå…¥: {use_spk_embedding}\n"
            f"ä½¿ç”¨æç¤ºè¯­éŸ³: {use_prompt_speech}\n"
            f"Mel cache length: {mel_cache_len}\n"
            f"è¾“å‡ºæ–‡ä»¶: {output_path}"
        )
        
        print("\n" + "="*60)
        print(info)
        print("="*60 + "\n")
        
        # ä¿®æ”¹ï¼šç›´æ¥è¿”å› output_path
        return get_audio_html(output_path), info, output_path        
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        print("\n" + "="*60)
        print(error_msg)
        print("="*60 + "\n")
        return None, error_msg, None


def process_audio_streaming(
    input_audio,
    reference_audio,
    block_size,
    max_token_len,
    use_spk_embedding = True,
    use_prompt_speech = True,
    mel_cache_len = 8
):
    """æµå¼éŸ³é¢‘å¤„ç†"""
    try:
        print("\n" + "="*60)
        print("å¼€å§‹æµå¼å¤„ç†")
        print("="*60)
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = initialize_model(mel_cache_len=mel_cache_len)
        
        if input_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ è¾“å…¥éŸ³é¢‘", None
        if reference_audio is None:
            return None, "âŒ è¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘", None
        
        # å‚æ•°æ£€æŸ¥
        token_mel_ratio = 4  # æ ¹æ®ä»£ç ä¸­çš„è®¾ç½®
        min_mel_cache = block_size * token_mel_ratio
        
        warning_msg = ""
        if mel_cache_len < min_mel_cache:
            warning_msg = f"âš ï¸ è­¦å‘Š: mel_cache_len ({mel_cache_len}) < block_size * token_mel_ratio ({min_mel_cache})\n"
            print(warning_msg)
        
        # å¤„ç†è¾“å…¥éŸ³é¢‘ï¼ˆé™åˆ¶æœ€å¤§30ç§’ï¼‰
        print("\n[STEP 1] å¤„ç†è¾“å…¥éŸ³é¢‘...")
        input_tensor, input_sr, input_duration = process_gradio_audio(input_audio, max_duration=30.0)
        temp_input = OUTPUT_DIR / "temp_input_stream.wav"
        torchaudio.save(str(temp_input), input_tensor, input_sr)
        print(f"[INFO] ä¸´æ—¶è¾“å…¥æ–‡ä»¶å·²ä¿å­˜: {temp_input}")
        
        # å¤„ç†å‚è€ƒéŸ³é¢‘ï¼ˆé™åˆ¶æœ€å¤§10ç§’ï¼‰
        print("\n[STEP 2] å¤„ç†å‚è€ƒéŸ³é¢‘...")
        ref_tensor, ref_sr, ref_duration = process_gradio_audio(reference_audio, max_duration=10.0)
        temp_reference = OUTPUT_DIR / "temp_reference_stream.wav"
        torchaudio.save(str(temp_reference), ref_tensor, ref_sr)
        print(f"[INFO] ä¸´æ—¶å‚è€ƒæ–‡ä»¶å·²ä¿å­˜: {temp_reference}")
        
        # ç¼–ç è¾“å…¥éŸ³é¢‘
        print("\n[STEP 3] æ­£åœ¨ç¼–ç éŸ³é¢‘...")
        audio_tokens = model.encode_token(str(temp_input))
        print(f"[INFO] âœ… ç”Ÿæˆäº† {len(audio_tokens)} ä¸ª tokens")
        
        # æµå¼è§£ç 
        print(f"\n[STEP 4] æ­£åœ¨è¿›è¡Œæµå¼è§£ç ...")
        print(f"[INFO] å‚æ•°:")
        print(f"  - block_size: {block_size}")
        print(f"  - max_token_len: {max_token_len if max_token_len else 'None (æ— é™åˆ¶)'}")
        print(f"  - mel_cache_len: {mel_cache_len}")
        print(f"  - use_spk_embedding: {use_spk_embedding}")
        print(f"  - use_prompt_speech: {use_prompt_speech}")
        
        result = model.decode_streaming(
            [audio_tokens],
            prompt_speech=str(temp_reference),
            use_spk_embedding=use_spk_embedding,
            use_prompt_speech=use_prompt_speech,
            block_size=block_size,
            max_token_len=max_token_len
        )
        
        # ä¿å­˜è¾“å‡º
        print(f"\n[STEP 5] ä¿å­˜è¾“å‡ºéŸ³é¢‘...")
        output_audio = result['syn_wav_list'][0]
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘å½¢çŠ¶: {output_audio.shape}")
        print(f"[INFO] è¾“å‡ºéŸ³é¢‘èŒƒå›´: [{output_audio.min():.6f}, {output_audio.max():.6f}]")
        
        output_path = save_audio_for_gradio(
            output_audio,
            sample_rate=24000,
            prefix="streaming_output"
        )
                
        if output_path is None:
            return None, "âŒ ä¿å­˜éŸ³é¢‘å¤±è´¥", None
        
        info = warning_msg
        info += f"âœ… æµå¼è§£ç å®Œæˆ\n"
        info += f"è¾“å…¥éŸ³é¢‘æ—¶é•¿: {input_duration:.2f}ç§’\n"
        info += f"å‚è€ƒéŸ³é¢‘æ—¶é•¿: {ref_duration:.2f}ç§’\n"
        info += f"Token æ•°é‡: {len(audio_tokens)}\n"
        info += f"Block size: {block_size}\n"
        info += f"Max token len: {max_token_len if max_token_len else 'None (æ— é™åˆ¶)'}\n"
        info += f"Mel cache length: {mel_cache_len}\n"
        info += f"ä½¿ç”¨è¯´è¯äººåµŒå…¥: {use_spk_embedding}\n"
        info += f"ä½¿ç”¨æç¤ºè¯­éŸ³: {use_prompt_speech}\n"
        info += f"è¾“å‡ºæ–‡ä»¶: {output_path}"
        
        print("\n" + "="*60)
        print(info)
        print("="*60 + "\n")
        
        # ä¿®æ”¹ï¼šç›´æ¥è¿”å› output_path
        return get_audio_html(output_path), info, output_path
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\nè¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}"
        print("\n" + "="*60)
        print(error_msg)
        print("="*60 + "\n")
        return None, error_msg, None


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    # å…ˆåˆå§‹åŒ–æ¨¡å‹
    model = initialize_model(mel_cache_len=8)
    
    with gr.Blocks(title="Streaming Codec å˜å£°å™¨æµ‹è¯•", theme=gr.themes.Soft()) as demo:
        gr.HTML(
            """
            <div style='text-align: center'>
                <h1>ğŸ™ï¸ Streaming Codec å˜å£°å™¨æµ‹è¯•</h1>
                <p>ä¸Šä¼ è¾“å…¥éŸ³é¢‘å’Œå‚è€ƒéŸ³é¢‘ï¼Œæµ‹è¯•éæµå¼å’Œæµå¼è§£ç æ•ˆæœ</p>
            </div>
            """
        )
        
        nonstream_state = gr.State(value=None)
        stream_state = gr.State(value=None)
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“¤ è¾“å…¥è®¾ç½®")
                
                input_audio = gr.Audio(
                    label="è¾“å…¥éŸ³é¢‘ (å¾…è½¬æ¢çš„éŸ³é¢‘)",
                    type="numpy",
                    sources=["upload", "microphone"]
                )
                
                reference_audio = gr.Audio(
                    label="å‚è€ƒéŸ³é¢‘ (ç›®æ ‡éŸ³è‰²)",
                    type="numpy",
                    sources=["upload", "microphone"]
                )
                
                # gr.Markdown("### âš™ï¸ å…±åŒå‚æ•°")
                
                mel_cache_len = 8
                use_spk_embedding = True
                use_prompt_speech = True
                # mel_cache_len = gr.Slider(
                #     minimum=4,
                #     maximum=16,
                #     step=1,
                #     value=8,
                #     label="Mel Cache Length",
                #     info="Vocoderè§£ç æ—¶overlapçš„é•¿åº¦ï¼Œå½±å“éŸ³è´¨è¿ç»­æ€§"
                # )
                
                # use_spk_embedding = gr.Checkbox(
                #     label="ä½¿ç”¨è¯´è¯äººåµŒå…¥",
                #     value=True
                # )
                # use_prompt_speech = gr.Checkbox(
                #     label="ä½¿ç”¨æç¤ºè¯­éŸ³",
                #     value=True
                # )
            
            with gr.Column(scale=1):
                gr.Markdown("### ğŸµ éæµå¼è§£ç ")
                
                with gr.Row():
                    nonstream_button = gr.Button(
                        "ğŸš€ éæµå¼è§£ç ",
                        variant="primary",
                        size="lg"
                    )
                
                nonstream_output = gr.HTML(label="éæµå¼è¾“å‡º")
                nonstream_reload = gr.Button("ğŸ” é‡æ–°åŠ è½½éæµå¼éŸ³é¢‘")
                nonstream_info = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=8,
                    interactive=False
                )
                
        with gr.Row():
            with gr.Column():
                gr.Markdown("### ğŸŒŠ æµå¼è§£ç ")
                
                with gr.Row():
                    block_size = gr.Slider(
                        minimum=2,
                        maximum=10,
                        step=1,
                        value=5,
                        label="Block Size",
                        info="æ¨ç†æ—¶æ¯ä¸€æ­¥ä¼šæ¨å‡ºçš„tokenæ•°"
                    )
                    
                    max_token_len = gr.Slider(
                        minimum=10,
                        maximum=50,
                        step=5,
                        value=15,
                        label="Max Token Length",
                        info="æ¯æ­¥é€å…¥flowæ¨¡å‹çš„æœ€å¤§tokenæ•°ï¼Œè®¾ä¸º0è¡¨ç¤ºä¸é™åˆ¶"
                    )
                
                with gr.Row():
                    stream_button = gr.Button(
                        "âš¡ æµå¼è§£ç ",
                        variant="primary",
                        size="lg"
                    )
                
                stream_output = gr.HTML(label="æµå¼è¾“å‡º")
                stream_reload = gr.Button("ğŸ” é‡æ–°åŠ è½½æµå¼éŸ³é¢‘")
                stream_info = gr.Textbox(
                    label="å¤„ç†ä¿¡æ¯",
                    lines=8,
                    interactive=False
                )
        
        # ç»‘å®šäº‹ä»¶
        nonstream_button.click(
            fn=process_audio_nonstreaming,
            inputs=[input_audio, reference_audio],
            outputs=[nonstream_output, nonstream_info, nonstream_state]
        )

        stream_button.click(
            fn=process_audio_streaming,
            inputs=[input_audio, reference_audio, block_size, max_token_len],
            outputs=[stream_output, stream_info, stream_state]
        )

        nonstream_reload.click(
            fn=reload_audio,
            inputs=[nonstream_state],
            outputs=[nonstream_output]
        )
        stream_reload.click(
            fn=reload_audio,
            inputs=[stream_state],
            outputs=[stream_output]
        )
        
        # ä½¿ç”¨æç¤º
        gr.Markdown("### ğŸ’¡ ä½¿ç”¨æç¤º")
        gr.Markdown(f"""
        1. **è¾“å…¥éŸ³é¢‘**: ä½ æƒ³è¦è½¬æ¢çš„åŸå§‹éŸ³é¢‘
        2. **å‚è€ƒéŸ³é¢‘**: ç›®æ ‡éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘
        3. **Mel Cache Length**: Vocoderè§£ç æ—¶overlapçš„é•¿åº¦ï¼Œå»ºè®® mel_cache_len / 4 â‰¤ block_size
        4. **éæµå¼è§£ç **: ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªéŸ³é¢‘ï¼Œè´¨é‡æ›´å¥½
        5. **æµå¼è§£ç **: åˆ†å—å¤„ç†éŸ³é¢‘ï¼Œå»¶è¿Ÿæ›´ä½ï¼Œé€‚åˆå®æ—¶åœºæ™¯
        6. **Block Size**: æ§åˆ¶æµå¼å¤„ç†çš„å—å¤§å° (2-10)
        7. **Max Token Length**: é™åˆ¶æ¯æ¬¡æ¨ç†çš„æœ€å¤§tokenæ•°é‡ (10-50)ï¼Œå¿…é¡» â‰¥ block_size + pre_lookahead_len(3) ï¼Œä¸”æœ€å¥½å¤šç•™æœ‰ä¸€äº›ä½™é‡
        8. **è¾“å‡ºç›®å½•**: `{OUTPUT_DIR}`
        
        **å‚æ•°å»ºè®®**:
        - mel_cache_len = 8, block_size = 5, max_token_len = 15 (é»˜è®¤é…ç½®)
        
        **é‡è¦æç¤º**: 
        - mel_cache_len / 4 åº”è¯¥ â‰¤ block_size
        - max_token_len å¿…é¡» â‰¥ block_size + pre_lookahead_len (é€šå¸¸ pre_lookahead_len â‰ˆ 5)
        """)
    
    return demo


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Streaming Codec Gradio Demo")
    parser.add_argument("--output_dir", type=str, default=None, help="æŒ‡å®šéŸ³é¢‘è¾“å‡ºç›®å½•")
    parser.add_argument("--port", type=int, default=7860, help="æŒ‡å®šæœåŠ¡ç«¯å£")
    args = parser.parse_args()

    # ç¡®å®šè¾“å‡ºç›®å½•: å‘½ä»¤è¡Œå‚æ•° > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
    if args.output_dir:
        OUTPUT_DIR = Path(args.output_dir).resolve()
    elif os.getenv("GRADIO_OUTPUT_DIR"):
        OUTPUT_DIR = Path(os.getenv("GRADIO_OUTPUT_DIR")).resolve()
    
    # åˆ›å»ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    print(f"è¾“å‡ºç›®å½• (ç»å¯¹è·¯å¾„): {OUTPUT_DIR}")
    print(f"æœåŠ¡ç«¯å£: {args.port}")

    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=args.port,
        share=False,
        allowed_paths=[str(OUTPUT_DIR)]
    )
    
    
    
    
